{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":191187,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":162966,"modelId":185327}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:36:33.275908Z","iopub.execute_input":"2024-12-07T05:36:33.276883Z","iopub.status.idle":"2024-12-07T05:36:34.388973Z","shell.execute_reply.started":"2024-12-07T05:36:33.276823Z","shell.execute_reply":"2024-12-07T05:36:34.387864Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/quora_classification/other/default/1/model (1).pkl\n/kaggle/input/quora_classification/other/default/1/cv.pkl\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import sklearn\nprint(sklearn.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T07:09:48.286437Z","iopub.execute_input":"2024-12-07T07:09:48.286826Z","iopub.status.idle":"2024-12-07T07:09:48.880457Z","shell.execute_reply.started":"2024-12-07T07:09:48.286793Z","shell.execute_reply":"2024-12-07T07:09:48.878782Z"}},"outputs":[{"name":"stdout","text":"1.2.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install distance\nimport bs4 \nimport re\nfrom bs4 import BeautifulSoup\nimport distance\nfrom fuzzywuzzy import fuzz\nimport pickle\nimport numpy as np\nimport nltk\nfrom nltk.corpus import stopwords\ncv = pickle.load(open('/kaggle/input/quora_classification/other/default/1/cv.pkl','rb'))\n\n\ndef test_common_words(q1,q2):\n    w1 = set(map(lambda word: word.lower().strip(), q1.split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), q2.split(\" \")))\n    return len(w1 & w2)\n\ndef test_total_words(q1,q2):\n    w1 = set(map(lambda word: word.lower().strip(), q1.split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), q2.split(\" \")))\n    return (len(w1) + len(w2))\n\n\ndef test_fetch_token_features(q1, q2):\n    SAFE_DIV = 0.0001\n\n    STOP_WORDS = stopwords.words(\"english\")\n\n    token_features = [0.0] * 8\n\n    # Converting the Sentence into Tokens:\n    q1_tokens = q1.split()\n    q2_tokens = q2.split()\n\n    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n        return token_features\n\n    # Get the non-stopwords in Questions\n    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n\n    # Get the stopwords in Questions\n    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n    q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])\n\n    # Get the common non-stopwords from Question pair\n    common_word_count = len(q1_words.intersection(q2_words))\n\n    # Get the common stopwords from Question pair\n    common_stop_count = len(q1_stops.intersection(q2_stops))\n\n    # Get the common Tokens from Question pair\n    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n\n    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)\n    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)\n    token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n    token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n\n    # Last word of both question is same or not\n    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])\n\n    # First word of both question is same or not\n    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n\n    return token_features\n\n\ndef test_fetch_length_features(q1, q2):\n    length_features = [0.0] * 3\n\n    # Converting the Sentence into Tokens:\n    q1_tokens = q1.split()\n    q2_tokens = q2.split()\n\n    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n        return length_features\n\n    # Absolute length features\n    length_features[0] = abs(len(q1_tokens) - len(q2_tokens))\n\n    # Average Token Length of both Questions\n    length_features[1] = (len(q1_tokens) + len(q2_tokens)) / 2\n\n    strs = list(distance.lcsubstrings(q1, q2))\n    length_features[2] = len(strs[0]) / (min(len(q1), len(q2)) + 1)\n\n    return length_features\n\n\ndef test_fetch_fuzzy_features(q1, q2):\n    fuzzy_features = [0.0] * 4\n\n    # fuzz_ratio\n    fuzzy_features[0] = fuzz.QRatio(q1, q2)\n\n    # fuzz_partial_ratio\n    fuzzy_features[1] = fuzz.partial_ratio(q1, q2)\n\n    # token_sort_ratio\n    fuzzy_features[2] = fuzz.token_sort_ratio(q1, q2)\n\n    # token_set_ratio\n    fuzzy_features[3] = fuzz.token_set_ratio(q1, q2)\n\n    return fuzzy_features\n\n\ndef preprocess(q):\n    q = str(q).lower().strip()\n\n    # Replace certain special characters with their string equivalents\n    q = q.replace('%', ' percent')\n    q = q.replace('$', ' dollar ')\n    q = q.replace('₹', ' rupee ')\n    q = q.replace('€', ' euro ')\n    q = q.replace('@', ' at ')\n\n    # The pattern '[math]' appears around 900 times in the whole dataset.\n    q = q.replace('[math]', '')\n\n    # Replacing some numbers with string equivalents (not perfect, can be done better to account for more cases)\n    q = q.replace(',000,000,000 ', 'b ')\n    q = q.replace(',000,000 ', 'm ')\n    q = q.replace(',000 ', 'k ')\n    q = re.sub(r'([0-9]+)000000000', r'\\1b', q)\n    q = re.sub(r'([0-9]+)000000', r'\\1m', q)\n    q = re.sub(r'([0-9]+)000', r'\\1k', q)\n\n    # Decontracting words\n    # https://en.wikipedia.org/wiki/Wikipedia%3aList_of_English_contractions\n    # https://stackoverflow.com/a/19794953\n    contractions = {\n        \"ain't\": \"am not\",\n        \"aren't\": \"are not\",\n        \"can't\": \"can not\",\n        \"can't've\": \"can not have\",\n        \"'cause\": \"because\",\n        \"could've\": \"could have\",\n        \"couldn't\": \"could not\",\n        \"couldn't've\": \"could not have\",\n        \"didn't\": \"did not\",\n        \"doesn't\": \"does not\",\n        \"don't\": \"do not\",\n        \"hadn't\": \"had not\",\n        \"hadn't've\": \"had not have\",\n        \"hasn't\": \"has not\",\n        \"haven't\": \"have not\",\n        \"he'd\": \"he would\",\n        \"he'd've\": \"he would have\",\n        \"he'll\": \"he will\",\n        \"he'll've\": \"he will have\",\n        \"he's\": \"he is\",\n        \"how'd\": \"how did\",\n        \"how'd'y\": \"how do you\",\n        \"how'll\": \"how will\",\n        \"how's\": \"how is\",\n        \"i'd\": \"i would\",\n        \"i'd've\": \"i would have\",\n        \"i'll\": \"i will\",\n        \"i'll've\": \"i will have\",\n        \"i'm\": \"i am\",\n        \"i've\": \"i have\",\n        \"isn't\": \"is not\",\n        \"it'd\": \"it would\",\n        \"it'd've\": \"it would have\",\n        \"it'll\": \"it will\",\n        \"it'll've\": \"it will have\",\n        \"it's\": \"it is\",\n        \"let's\": \"let us\",\n        \"ma'am\": \"madam\",\n        \"mayn't\": \"may not\",\n        \"might've\": \"might have\",\n        \"mightn't\": \"might not\",\n        \"mightn't've\": \"might not have\",\n        \"must've\": \"must have\",\n        \"mustn't\": \"must not\",\n        \"mustn't've\": \"must not have\",\n        \"needn't\": \"need not\",\n        \"needn't've\": \"need not have\",\n        \"o'clock\": \"of the clock\",\n        \"oughtn't\": \"ought not\",\n        \"oughtn't've\": \"ought not have\",\n        \"shan't\": \"shall not\",\n        \"sha'n't\": \"shall not\",\n        \"shan't've\": \"shall not have\",\n        \"she'd\": \"she would\",\n        \"she'd've\": \"she would have\",\n        \"she'll\": \"she will\",\n        \"she'll've\": \"she will have\",\n        \"she's\": \"she is\",\n        \"should've\": \"should have\",\n        \"shouldn't\": \"should not\",\n        \"shouldn't've\": \"should not have\",\n        \"so've\": \"so have\",\n        \"so's\": \"so as\",\n        \"that'd\": \"that would\",\n        \"that'd've\": \"that would have\",\n        \"that's\": \"that is\",\n        \"there'd\": \"there would\",\n        \"there'd've\": \"there would have\",\n        \"there's\": \"there is\",\n        \"they'd\": \"they would\",\n        \"they'd've\": \"they would have\",\n        \"they'll\": \"they will\",\n        \"they'll've\": \"they will have\",\n        \"they're\": \"they are\",\n        \"they've\": \"they have\",\n        \"to've\": \"to have\",\n        \"wasn't\": \"was not\",\n        \"we'd\": \"we would\",\n        \"we'd've\": \"we would have\",\n        \"we'll\": \"we will\",\n        \"we'll've\": \"we will have\",\n        \"we're\": \"we are\",\n        \"we've\": \"we have\",\n        \"weren't\": \"were not\",\n        \"what'll\": \"what will\",\n        \"what'll've\": \"what will have\",\n        \"what're\": \"what are\",\n        \"what's\": \"what is\",\n        \"what've\": \"what have\",\n        \"when's\": \"when is\",\n        \"when've\": \"when have\",\n        \"where'd\": \"where did\",\n        \"where's\": \"where is\",\n        \"where've\": \"where have\",\n        \"who'll\": \"who will\",\n        \"who'll've\": \"who will have\",\n        \"who's\": \"who is\",\n        \"who've\": \"who have\",\n        \"why's\": \"why is\",\n        \"why've\": \"why have\",\n        \"will've\": \"will have\",\n        \"won't\": \"will not\",\n        \"won't've\": \"will not have\",\n        \"would've\": \"would have\",\n        \"wouldn't\": \"would not\",\n        \"wouldn't've\": \"would not have\",\n        \"y'all\": \"you all\",\n        \"y'all'd\": \"you all would\",\n        \"y'all'd've\": \"you all would have\",\n        \"y'all're\": \"you all are\",\n        \"y'all've\": \"you all have\",\n        \"you'd\": \"you would\",\n        \"you'd've\": \"you would have\",\n        \"you'll\": \"you will\",\n        \"you'll've\": \"you will have\",\n        \"you're\": \"you are\",\n        \"you've\": \"you have\"\n    }\n\n    q_decontracted = []\n\n    for word in q.split():\n        if word in contractions:\n            word = contractions[word]\n\n        q_decontracted.append(word)\n\n    q = ' '.join(q_decontracted)\n    q = q.replace(\"'ve\", \" have\")\n    q = q.replace(\"n't\", \" not\")\n    q = q.replace(\"'re\", \" are\")\n    q = q.replace(\"'ll\", \" will\")\n\n    # Removing HTML tags\n    q = BeautifulSoup(q)\n    q = q.get_text()\n\n    # Remove punctuations\n    pattern = re.compile('\\W')\n    q = re.sub(pattern, ' ', q).strip()\n\n    return q\n\n\ndef query_point_creator(q1, q2):\n    input_query = []\n\n    # preprocess\n    q1 = preprocess(q1)\n    q2 = preprocess(q2)\n\n    # fetch basic features\n    input_query.append(len(q1))\n    input_query.append(len(q2))\n\n    input_query.append(len(q1.split(\" \")))\n    input_query.append(len(q2.split(\" \")))\n\n    input_query.append(test_common_words(q1, q2))\n    input_query.append(test_total_words(q1, q2))\n    input_query.append(round(test_common_words(q1, q2) / test_total_words(q1, q2), 2))\n\n    # fetch token features\n    token_features = test_fetch_token_features(q1, q2)\n    input_query.extend(token_features)\n\n    # fetch length based features\n    length_features = test_fetch_length_features(q1, q2)\n    input_query.extend(length_features)\n\n    # fetch fuzzy features\n    fuzzy_features = test_fetch_fuzzy_features(q1, q2)\n    input_query.extend(fuzzy_features)\n\n    # bow feature for q1\n    q1_bow = cv.transform([q1]).toarray()\n\n    # bow feature for q2\n    q2_bow = cv.transform([q2]).toarray()\n\n    return np.hstack((np.array(input_query).reshape(1, 22), q1_bow, q2_bow))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T06:19:39.135718Z","iopub.execute_input":"2024-12-07T06:19:39.136278Z","iopub.status.idle":"2024-12-07T06:19:49.239272Z","shell.execute_reply.started":"2024-12-07T06:19:39.136229Z","shell.execute_reply":"2024-12-07T06:19:49.237875Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: distance in /opt/conda/lib/python3.10/site-packages (0.1.3)\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"!pip install gradio ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T06:19:49.241594Z","iopub.execute_input":"2024-12-07T06:19:49.241987Z","iopub.status.idle":"2024-12-07T06:19:59.582900Z","shell.execute_reply.started":"2024-12-07T06:19:49.241944Z","shell.execute_reply":"2024-12-07T06:19:59.581398Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: gradio in /opt/conda/lib/python3.10/site-packages (5.8.0)\nRequirement already satisfied: aiofiles<24.0,>=22.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (22.1.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (4.4.0)\nRequirement already satisfied: fastapi<1.0,>=0.115.2 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.115.6)\nRequirement already satisfied: ffmpy in /opt/conda/lib/python3.10/site-packages (from gradio) (0.4.0)\nRequirement already satisfied: gradio-client==1.5.1 in /opt/conda/lib/python3.10/site-packages (from gradio) (1.5.1)\nRequirement already satisfied: httpx>=0.24.1 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.27.0)\nRequirement already satisfied: huggingface-hub>=0.25.1 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.26.2)\nRequirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.1.4)\nRequirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.1.5)\nRequirement already satisfied: numpy<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (1.26.4)\nRequirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.10.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from gradio) (21.3)\nRequirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.2.3)\nRequirement already satisfied: pillow<12.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (10.3.0)\nRequirement already satisfied: pydantic>=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.10.2)\nRequirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from gradio) (0.25.1)\nRequirement already satisfied: python-multipart>=0.0.18 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.0.19)\nRequirement already satisfied: pyyaml<7.0,>=5.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (6.0.2)\nRequirement already satisfied: ruff>=0.2.2 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.8.2)\nRequirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.1.6)\nRequirement already satisfied: semantic-version~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.10.0)\nRequirement already satisfied: starlette<1.0,>=0.40.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.41.3)\nRequirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.13.2)\nRequirement already satisfied: typer<1.0,>=0.12 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.12.3)\nRequirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (4.12.2)\nRequirement already satisfied: uvicorn>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.30.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from gradio-client==1.5.1->gradio) (2024.9.0)\nRequirement already satisfied: websockets<15.0,>=10.0 in /opt/conda/lib/python3.10/site-packages (from gradio-client==1.5.1->gradio) (12.0)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (3.7)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.2.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (2024.6.2)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (1.0.5)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.1->gradio) (3.15.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.1->gradio) (4.66.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->gradio) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.1 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (2.27.1)\nRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.3.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.1->gradio) (1.26.18)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import gradio as gr\nimport pickle\nimport helper\n\n# Load the model\nmodel = pickle.load(open('/kaggle/input/quora_classification/other/default/1/model (1).pkl', 'rb'))\n\n# Define the prediction function\ndef find_duplicate(q1, q2):\n    query = query_point_creator(q1, q2)\n    result = model.predict(query)[0]\n    return \"Duplicate\" if result else \"Not Duplicate\"\n\n# Create the Gradio interface\ninterface = gr.Interface(\n    fn=find_duplicate,\n    inputs=[\n        gr.Textbox(label=\"Enter Question 1\"),\n        gr.Textbox(label=\"Enter Question 2\")\n    ],\n    outputs=gr.Textbox(label=\"Result\"),\n    title=\"Duplicate Question Pairs\",\n    description=\"Enter two questions to check if they are duplicates.\"\n)\n\n# Launch the interface\ninterface.launch()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T06:19:59.584767Z","iopub.execute_input":"2024-12-07T06:19:59.585253Z","iopub.status.idle":"2024-12-07T06:20:00.403539Z","shell.execute_reply.started":"2024-12-07T06:19:59.585174Z","shell.execute_reply":"2024-12-07T06:20:00.402078Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7862\nKaggle notebooks require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://327163f587261d5069.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://327163f587261d5069.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"import fuzzywuzzy\nprint(fuzzywuzzy.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T07:14:13.520991Z","iopub.execute_input":"2024-12-07T07:14:13.521386Z","iopub.status.idle":"2024-12-07T07:14:13.529357Z","shell.execute_reply.started":"2024-12-07T07:14:13.521343Z","shell.execute_reply":"2024-12-07T07:14:13.528032Z"}},"outputs":[{"name":"stdout","text":"0.18.0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"! pip install distance","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T07:15:04.120776Z","iopub.execute_input":"2024-12-07T07:15:04.121167Z","iopub.status.idle":"2024-12-07T07:15:18.759987Z","shell.execute_reply.started":"2024-12-07T07:15:04.121132Z","shell.execute_reply":"2024-12-07T07:15:18.758450Z"}},"outputs":[{"name":"stdout","text":"Collecting distance\n  Downloading Distance-0.1.3.tar.gz (180 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: distance\n  Building wheel for distance (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for distance: filename=Distance-0.1.3-py3-none-any.whl size=16258 sha256=dc9edfb7f11645c2869f2efd30ec7b1a292e35ca31254b6b27e1afdcefa115bc\n  Stored in directory: /root/.cache/pip/wheels/e8/bb/de/f71bf63559ea9a921059a5405806f7ff6ed612a9231c4a9309\nSuccessfully built distance\nInstalling collected packages: distance\nSuccessfully installed distance-0.1.3\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import nltk\nprint(nltk.__version__) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T07:15:40.616842Z","iopub.execute_input":"2024-12-07T07:15:40.617206Z","iopub.status.idle":"2024-12-07T07:15:41.567964Z","shell.execute_reply.started":"2024-12-07T07:15:40.617176Z","shell.execute_reply":"2024-12-07T07:15:41.567022Z"}},"outputs":[{"name":"stdout","text":"3.2.4\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import numpy as np\nprint(np.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T07:18:58.477105Z","iopub.execute_input":"2024-12-07T07:18:58.477518Z","iopub.status.idle":"2024-12-07T07:18:58.483509Z","shell.execute_reply.started":"2024-12-07T07:18:58.477484Z","shell.execute_reply":"2024-12-07T07:18:58.482217Z"}},"outputs":[{"name":"stdout","text":"1.26.4\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import sys\nprint(sys.version)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T07:26:03.915094Z","iopub.execute_input":"2024-12-07T07:26:03.915555Z","iopub.status.idle":"2024-12-07T07:26:03.921792Z","shell.execute_reply.started":"2024-12-07T07:26:03.915518Z","shell.execute_reply":"2024-12-07T07:26:03.920526Z"}},"outputs":[{"name":"stdout","text":"3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0]\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}